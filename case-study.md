# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумала использовать такую метрику:

Измерила (вариант с изменениями из первого задания) с помощью предложенной команды:

`puts "MEMORY USAGE: %d MB" % (`ps -o rss= -p #{Process.pid}`.to_i / 1024)`

Результат в начале:

```
# с GC

MEMORY USAGE: 2444 MB

# без GC

MEMORY USAGE: 5924 MB
```

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроила эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 35 секунд.


Вот как я построил `feedback_loop`:

Прописала, чтобы оценивать кол-во памяти:
```ruby
puts "MEMORY USAGE: %d MB" % (`ps -o rss= -p #{Process.pid}`.to_i / 1024)
```

На этапе отладки потокового варианта использовала sample, к-й позволял оценить работоспособность в пределах пары секунд.

Далее подобрала sample, с к-м фидбек можно было получить за n секунд.


Далее использовала простой вывод, чтобы оценивать в пределах 35 секунд.

Либо sample на 1_000_000 строк, тогда в пределах 7 секунд.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался *инструментами, которыми вы воспользовались*

Вот какие проблемы удалось найти и решить

### Ваша находка №0

Переход на потоковую обработку.

По времени программа стала работать медленнее (25с => 33-34c , с gc)

По памяти произошло улучшение:

`MEMORY USAGE: 2444 MB`
=>
`MEMORY USAGE: 332 MB`

Думала использовать `gem 'json-write-stream'` , но неудобно писать вложенный json для `usersStats`, поэтому решила напрямую

### Ваша находка №1

memory profiler показал ,что File очень много ест памяти, но хз, как от него избавиться!

Также посмотрела qcachegrind (у меня kcachegrind) - много отнимают `write_user_to_json` , `Array#each`.

Также создала второй тред, вот его отчёт:

```
MEMORY USAGE: 29 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
MEMORY USAGE: 334 MB
22.006778048
```


Подумала, раз `File`, то не открывать/закрывать его для каждого юзера, а открыть на запись 1 раз.

Переписала - по памяти ничего не поменялось, но программа стала быстрее отрабатывать (27 => 21 секунд, если проверять на большом файле без профилирования)

### Ваша находка №2

Далее снова посмотрела с помощью memory_profiler:

Увидела, что String ест много памяти, причём на месте `split(',')`

Заметила, что при проверке строки "юзер" или "сессия" вообще необязательно split делать. Но также заметила, что split делается 2 раза для каждой строки: для проверки user / session и для парсинга юзера/сессии.

Сначала переписала на разовый `split` и передачу `cols` в `parse_session` и `parse_user`.

```ruby
cols = line.split(',')
...
parse_user(cols)
```

Использование памяти на sample снизилось с:
```
Total allocated: 179.51 MB (2546193 objects)
```
До:

```
Total allocated: 131.29 MB (1761624 objects)
```

Далее избавилась от массива в `split` (у меня это уже было в предыдущем варианте, заметила, что вариант с набором переменных быстрее отрабатывает).
+ избавилась от методов пока что `parse_session`, `parse_user`.

```ruby
_, user_id, session_id, browser, time, date = cols.split(',')

session = {
  'user_id' => user_id,
  'session_id' => session_id,
  'browser' => browser,
  'time' => time,
  'date' => date,
}
```

Использование памяти на сэмпле снизилось до:
```
Total allocated: 51.71 MB (854936 objects)
```

Проверяю основную метрику на большом файле:

```
MEMORY USAGE: 33 MB
MEMORY USAGE: 327 MB
MEMORY USAGE: 327 MB
MEMORY USAGE: 327 MB
MEMORY USAGE: 327 MB
MEMORY USAGE: 327 MB
MEMORY USAGE: 327 MB
MEMORY USAGE: 327 MB
7.241316391
```

Общий объём использованной памяти немного снизился + программа стала отрабатывать намного быстрее!


### Ваша находка №X
- какой отчёт показал главную точку роста
- как вы решили её оптимизировать
- как изменилась метрика
- как изменился отчёт профилировщика

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*
