# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла проблема — программа для обработки txt-файла потребляла большой объём памяти. Например, для обработки файла ~ 130 mB (~ 3_000_000 строк) использовалось > 1_190 mB памяти.

Я решила оптимизировать программу и снизить потребление памяти, установив бюджет 70 mB на обработку файлов до 150 mB.

## Формирование метрики
Для того, чтобы понимать, как влияют ли мои изменения на быстродействие программы, я решила оценивать объём потребляемой памяти при обработке файла 50_000 строк (2 mB).

Фиксируем потребление памяти до оптимизации:
32 mB

## Гарантия корректности работы оптимизированной программы
Чтобы предотвратить изменение логики программы при оптимизации, я использовала тест. Его выполнение в фидбек-лупе гарантировало, что на выходе программа по-прежнему генерирует корректный json-объект.

## Feedback-Loop
Для того, чтобы проверять свои гипотезы максимально быстро, я выстроил следующий feedback-loop:
— профилирование программы (гем MemoryProfiler/Valgrind Massif, RubyProf)      
                                                                     ~ 10 sec
— изменение кода
→ Тестирование работоспособности программы (MiniTest)                ~ 1 sec
→ Тестирование метрики (выполнение программы с выводом
  использованного количестова памяти в конце)                        ~ 20 sec

В итоге фидбек-луп позволил мне получать обратную связь по эффективности сделанных изменений за 30 секунд.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я использовала гем MemoryProfiler.

Вот какие проблемы удалось найти и решить.

### Избыточное хранение данных в памяти
Из отчёта MemoryProfiler видно, что 99.9% памяти во время работы уходят на хранение данных в строках, массивах и хэше.

32.92 MB  String
17.92 MB  Array
 3.05 MB  Hash (!!!)

На этом этапе мы можем избавиться от самого большого хэша, размер которого растёт пропорционально размеру файла с данными, т.к. именно в него мы «складываем» обработанные строки txt-файла перед тем, как записать result.json.

Реализуем потоковую обработку данных: построчное чтение информации, обработка блока (1 пользователь + его сессии) и запись данных в файл с результатами.

Потребление памяти снизилось на 20% (до 25 mB).

Распределение памяти по типам объектов приняло вид:

28.05 MB  String
14.06 MB  Array
 7.43 kB  Hash (!!!)

### Использование памяти растёт в течение всего времени выполнения программы
Отчёт Valgrind Massif показал, что в течение всего времени выполнения программы используемое количество памяти растёт.

![massif1](https://sun9-33.userapi.com/c850608/v850608800/c0fe9/WUGx8-LVB64.jpg)

По идеe такой ситуации не должно быть — если программа работает в потоковом режиме, то прирост потребления памяти должен быть незначительным или вовсе отсутствовать.

Ищем, где сливается память. RubyProf показывает, что критичное место в программе одно — String#split. В этом месте создаётся массив со строками часть которых, скорее всего, не удаляется GC.

Единственное, что сохраняется во время работы всй программы — это массив строк с информацией о каждом браузере. Создаваться эти строки будут в любом случае, но мы можем хранить не все подряд, а только уникальные, уменьшив потребление памяти (браузеров же не так много разных на самом деле, наверняка в массиве есть повторяющиеся строки — сотни тысяч :).

Меняем массив браузеров на Set и прогоняем Valgrind Massif. Отчёт стал ровным.

![massif2](https://sun9-48.userapi.com/c850608/v850608800/c0ff2/c5vN8jDNQog.jpg)

Теперь программа использует для работы 15 mB памяти. Исходя из того, что график Valgrind Massif стал ровным, на большом количестве данных количество памяти останется равным 15 mB.

## Результаты
Файл на 3_000_000 строк обрабатывается с импользованием минимального количества памяти (15 mB). Также можно утверждать, что и бОльший файл программа обработает, используя то же или чуть большее количество памяти.

Программа стала не только потреблять меньше памяти, но и работать быстрее. Прошлый результаи — 32 секунды, текущий — 29 секунд.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях кода я добавила в конец программы возврат количества используемой памяти, и тест, который сравнивает это количество с 20 mB (с запасом) :)
```ruby
assert_operator 20, :>=, work('data_large.txt')
```